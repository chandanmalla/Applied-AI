{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Fine Food Reviews Analysis\n",
    "Data Source: https://www.kaggle.com/snap/amazon-fine-food-reviews \n",
    "\n",
    "EDA: https://nycdatascience.com/blog/student-works/amazon-fine-foods-visualization/\n",
    "\n",
    "The Amazon Fine Food Reviews dataset consists of reviews of fine foods from Amazon.\n",
    "\n",
    "Number of reviews: 568,454\n",
    "Number of users: 256,059\n",
    "Number of products: 74,258\n",
    "Timespan: Oct 1999 - Oct 2012\n",
    "Number of Attributes/Columns in data: 10\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "1. Id\n",
    "2. ProductId - unique identifier for the product\n",
    "3. UserId - unqiue identifier for the user\n",
    "4. ProfileName\n",
    "5. HelpfulnessNumerator - number of users who found the review helpful\n",
    "6. HelpfulnessDenominator - number of users who indicated whether they found the review helpful or not\n",
    "7. Score - rating between 1 and 5\n",
    "8. Time - timestamp for the review\n",
    "9. Summary - brief summary of the review\n",
    "10. Text - text of the review\n",
    "Objective:\n",
    "Given a review, determine whether the review is positive (Rating of 4 or 5) or negative (rating of 1 or 2).\n",
    "\n",
    "\n",
    "[Q] How to determine if a review is positive or negative?\n",
    "\n",
    "[Ans] We could use the Score/Rating. A rating of 4 or 5 could be cosnidered a positive review. A review of 1 or 2 could be considered negative. A review of 3 is nuetral and ignored. This is an approximate and proxy way of determining the polarity (positivity/negativity) of a review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "The dataset is available in two forms\n",
    "\n",
    ".csv file\n",
    "SQLite Database\n",
    "In order to load the data, We have used the SQLITE dataset as it easier to query the data and visualise the data efficiently. \n",
    "\n",
    "\n",
    "Here as we only want to get the global sentiment of the recommendations (positive or negative), we will purposefully ignore all Scores equal to 3. If the score id above 3, then the recommendation wil be set to \"positive\". Otherwise, it will be set to \"negative\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#block for all Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making connection with Database\n",
    "con = sqlite3.connect('./database.sqlite')\n",
    "\n",
    "##fetching Data\n",
    "\n",
    "filtered_data = pd.read_sql_query(\"SELECT * from reviews where Score != 3\",con)\n",
    "\n",
    "##Taking only 10000 documents\n",
    "filtered_data=filtered_data.iloc[0:10000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = filtered_data['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      1  1303862400   \n",
       "1                     0                       0      0  1346976000   \n",
       "2                     1                       1      1  1219017600   \n",
       "3                     3                       3      0  1307923200   \n",
       "4                     0                       0      1  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##To arrange score as only positive and negative set of scores we set all the score below 3 as negative and above as positive\n",
    "\n",
    "# One way to do that:\n",
    "#filtered_data.loc[:,['Score']]>3 = 1\n",
    "#filtered_data.loc[:,['Score']]<3 = 0\n",
    "\n",
    "# Another way\n",
    "\n",
    "\n",
    "temp = temp.map(lambda x:1 if x>3 else 0)\n",
    "filtered_data['Score'] = temp\n",
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data De-duplication(pandas drop_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>count(*)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>#oc-R115TNMSPFT9I7</td>\n",
       "      <td>2</td>\n",
       "      <td>1331510400</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>#oc-R11D9D7SHXIJB9</td>\n",
       "      <td>5</td>\n",
       "      <td>1342396800</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>#oc-R11DNU2NBKQ23Z</td>\n",
       "      <td>1</td>\n",
       "      <td>1348531200</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>#oc-R11O5J5ZVQE25C</td>\n",
       "      <td>5</td>\n",
       "      <td>1346889600</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>#oc-R12KPBODL2B5ZD</td>\n",
       "      <td>1</td>\n",
       "      <td>1348617600</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70863</td>\n",
       "      <td>AZZJDUEFXYXBM</td>\n",
       "      <td>4</td>\n",
       "      <td>1284163200</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70864</td>\n",
       "      <td>AZZNK89PXD006</td>\n",
       "      <td>5</td>\n",
       "      <td>1269648000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70865</td>\n",
       "      <td>AZZTH6DJ0KSIP</td>\n",
       "      <td>5</td>\n",
       "      <td>1304208000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70866</td>\n",
       "      <td>AZZU1VEO8KUXH</td>\n",
       "      <td>5</td>\n",
       "      <td>1317513600</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70867</td>\n",
       "      <td>AZZU4D6TZ2L6J</td>\n",
       "      <td>5</td>\n",
       "      <td>1247875200</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70868 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   UserId  Score        Time  count(*)\n",
       "0      #oc-R115TNMSPFT9I7      2  1331510400         2\n",
       "1      #oc-R11D9D7SHXIJB9      5  1342396800         3\n",
       "2      #oc-R11DNU2NBKQ23Z      1  1348531200         2\n",
       "3      #oc-R11O5J5ZVQE25C      5  1346889600         3\n",
       "4      #oc-R12KPBODL2B5ZD      1  1348617600         2\n",
       "...                   ...    ...         ...       ...\n",
       "70863       AZZJDUEFXYXBM      4  1284163200         4\n",
       "70864       AZZNK89PXD006      5  1269648000         2\n",
       "70865       AZZTH6DJ0KSIP      5  1304208000         2\n",
       "70866       AZZU1VEO8KUXH      5  1317513600         3\n",
       "70867       AZZU4D6TZ2L6J      5  1247875200         2\n",
       "\n",
       "[70868 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets find if there are any duplicate\n",
    "###display = pd.read_sql_query(\" Select * from reviews r1  where Score != 3 and r1.productid = productid\",con)\n",
    "#No duplicate in above query\n",
    "#There are duplicates as there can not be multiple review from the user at same time and text being also same\n",
    "display = pd.read_sql_query(\"\"\"\n",
    "Select userid,score,time,count(*) from reviews r1  \n",
    "group by userid,score,time having count(*) > 1 \"\"\",con)\n",
    "display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>113682</td>\n",
       "      <td>B001O2HBIM</td>\n",
       "      <td>AZZJDUEFXYXBM</td>\n",
       "      <td>J. Lewis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1284163200</td>\n",
       "      <td>Recommend</td>\n",
       "      <td>My 6.5 month son enjoyed this flavor and it he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>386128</td>\n",
       "      <td>B000ER3EAM</td>\n",
       "      <td>AZZJDUEFXYXBM</td>\n",
       "      <td>J. Lewis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1284163200</td>\n",
       "      <td>Recommend</td>\n",
       "      <td>My 6.5 month son enjoyed this flavor and it he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>446101</td>\n",
       "      <td>B001BM6NIY</td>\n",
       "      <td>AZZJDUEFXYXBM</td>\n",
       "      <td>J. Lewis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1284163200</td>\n",
       "      <td>Recommend</td>\n",
       "      <td>My 6.5 month son enjoyed this flavor and it he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>506451</td>\n",
       "      <td>B000ER5D9W</td>\n",
       "      <td>AZZJDUEFXYXBM</td>\n",
       "      <td>J. Lewis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1284163200</td>\n",
       "      <td>Recommend</td>\n",
       "      <td>My 6.5 month son enjoyed this flavor and it he...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id   ProductId         UserId ProfileName  HelpfulnessNumerator  \\\n",
       "0  113682  B001O2HBIM  AZZJDUEFXYXBM    J. Lewis                     0   \n",
       "1  386128  B000ER3EAM  AZZJDUEFXYXBM    J. Lewis                     0   \n",
       "2  446101  B001BM6NIY  AZZJDUEFXYXBM    J. Lewis                     0   \n",
       "3  506451  B000ER5D9W  AZZJDUEFXYXBM    J. Lewis                     0   \n",
       "\n",
       "   HelpfulnessDenominator  Score        Time    Summary  \\\n",
       "0                       0      4  1284163200  Recommend   \n",
       "1                       0      4  1284163200  Recommend   \n",
       "2                       0      4  1284163200  Recommend   \n",
       "3                       0      4  1284163200  Recommend   \n",
       "\n",
       "                                                Text  \n",
       "0  My 6.5 month son enjoyed this flavor and it he...  \n",
       "1  My 6.5 month son enjoyed this flavor and it he...  \n",
       "2  My 6.5 month son enjoyed this flavor and it he...  \n",
       "3  My 6.5 month son enjoyed this flavor and it he...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display = pd.read_sql_query(\"\"\"\n",
    "Select * from reviews r1  \n",
    "where userid='AZZJDUEFXYXBM' \"\"\",con)\n",
    "display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted_data=filtered_data.sort_values('ProductId', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
      "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(filtered_data.columns)\n",
    "final = filtered_data.drop_duplicates({'UserId','ProfileName', 'Time','Text'},keep = 'first' , inplace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9564, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Size of remaining data\n",
    "final.shape[0]/filtered_data.shape[0]\n",
    "\n",
    "final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for Data inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>44737</td>\n",
       "      <td>B001EQ55RW</td>\n",
       "      <td>A2V0I904FH7ABY</td>\n",
       "      <td>Ram</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1212883200</td>\n",
       "      <td>Pure cocoa taste with crunchy almonds inside</td>\n",
       "      <td>It was almost a 'love at first bite' - the per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>64422</td>\n",
       "      <td>B000MIDROQ</td>\n",
       "      <td>A161DK06JJMCYF</td>\n",
       "      <td>J. E. Stephens \"Jeanne\"</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1224892800</td>\n",
       "      <td>Bought This for My Son at College</td>\n",
       "      <td>My son loves spaghetti so I didn't hesitate or...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id   ProductId          UserId              ProfileName  \\\n",
       "0  44737  B001EQ55RW  A2V0I904FH7ABY                      Ram   \n",
       "1  64422  B000MIDROQ  A161DK06JJMCYF  J. E. Stephens \"Jeanne\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     3                       2      4  1212883200   \n",
       "1                     3                       1      5  1224892800   \n",
       "\n",
       "                                        Summary  \\\n",
       "0  Pure cocoa taste with crunchy almonds inside   \n",
       "1             Bought This for My Son at College   \n",
       "\n",
       "                                                Text  \n",
       "0  It was almost a 'love at first bite' - the per...  \n",
       "1  My son loves spaghetti so I didn't hesitate or...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display = pd.read_sql_query(\"\"\"\n",
    "Select * from reviews r1  \n",
    "where HelpfulnessNumerator> HelpfulnessDenominator \"\"\",con)\n",
    "\n",
    "##There are data inconsistencies\n",
    "display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##there are lot of to deal with it, one of the way is\n",
    "\n",
    "final = final[final['HelpfulnessNumerator']<=final['HelpfulnessDenominator']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9564"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Size of remaining data\n",
    "final.shape[0]/filtered_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    7976\n",
       "0    1588\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##checking the priors\n",
    "final['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [3].  Text Preprocessing.\n",
    "\n",
    "Now that we have finished deduplication our data requires some preprocessing before we go on further with analysis and making the prediction model.\n",
    "\n",
    "Hence in the Preprocessing phase we do the following in the order below:-\n",
    "\n",
    "1. Begin by removing the html tags\n",
    "2. Remove any punctuations or limited set of special characters like , or . or # etc.\n",
    "3. Check if the word is made up of english letters and is not alpha-numeric\n",
    "4. Check to see if the length of the word is greater than 2 (as it was researched that there is no adjective in 2-letters)\n",
    "5. Convert the word to lowercase\n",
    "6. Remove Stopwords\n",
    "7. Finally Snowball Stemming the word (it was obsereved to be better than Porter Stemming)<br>\n",
    "\n",
    "After which we collect the words used to describe positive and negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### regular Expression https://pymotw.com/2/re\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "##Remove all http links\n",
    "def remhttp(text): \n",
    "    text = re.sub(r'http\\S+','',text) ##\\S is all the non-whitespace character + - one or more\n",
    "    return text\n",
    "\n",
    "##Remove all html tags\n",
    "def remhtml(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    text = soup.get_text()\n",
    "    return text\n",
    "\n",
    "##Remove all punct. or special characters\n",
    "def remchar(text):\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]+','',text)\n",
    "    return text\n",
    "\n",
    "##Remove all words less than 3 letters\n",
    "def remles2letter(text):\n",
    "    text = re.sub(r'\\W*\\b\\w{1,3}\\b','',text)\n",
    "    return text\n",
    "##convert to lower\n",
    "def lower(text):\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "\n",
    "def allconvert(text):\n",
    "    return lower(remles2letter(remchar(remhtml(remhttp(text)))))\n",
    "final['Score'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'have', 'why', 'such', \"shouldn't\", 'doesn', 'shan', 'then', 'will', 'shouldn', 'i', 'because', 'with', 'so', 'ma', 'll', 'weren', 'me', 'd', \"weren't\", 'and', 'for', 'yourself', 'after', 'between', 'just', \"that'll\", 'isn', \"won't\", 'being', 'can', 'to', \"you've\", 'than', 'been', 'himself', 'very', 'they', 'those', 'my', 'up', 'of', 'themselves', 'won', 'same', 'their', 'has', 'itself', \"she's\", 'your', 'ourselves', 'any', \"you're\", 'over', 'not', \"wasn't\", 'own', 'against', 'during', 'do', 'where', 'or', 'he', \"hadn't\", 'we', 'wouldn', 'but', 'by', 'y', \"hasn't\", 'needn', 'nor', 'she', 'again', \"you'd\", 'be', 'if', 'about', 'o', 'too', 'few', \"you'll\", 'how', 'until', 'through', 'them', 'above', 'yourselves', 'each', \"should've\", 'haven', 'are', 'mustn', 'off', 'whom', 'don', \"it's\", 've', 'these', 'at', 'on', \"mustn't\", 'is', 'which', 'hadn', 'that', 'from', 'his', 'under', 'were', \"mightn't\", 'all', 'ours', 'who', 't', 'the', 'most', 'our', \"doesn't\", 'in', 'having', 'her', 'mightn', 'an', \"haven't\", 'when', 'should', 'theirs', 'am', 'no', \"needn't\", 'was', 'once', 'its', 'couldn', 'yours', 'it', 'as', 'did', \"isn't\", 'below', 'what', 'while', 'now', 'myself', 'herself', \"wouldn't\", 'this', \"don't\", 'more', 'before', 'further', 'hers', 'doing', 'm', 'both', 'aren', 'down', 'other', \"couldn't\", 'into', 're', \"didn't\", 's', 'ain', \"aren't\", 'out', 'you', 'didn', 'here', 'there', 'him', 'had', 'some', 'only', \"shan't\", 'a', 'hasn', 'wasn', 'does'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "#import nltk\n",
    "#nltk.download('stopwords')                                     download stopwords\n",
    "\n",
    "#stop = set(stopwords.words('english'))\n",
    "\n",
    "##we can use set stopwords manually also.\n",
    "stop= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"])\n",
    "sno = SnowballStemmer('english')\n",
    "stop = set(stopwords.words('english'))\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected a bytes-like object, str found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-04b09092bb5e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34mb' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mfinal_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 0: expected a bytes-like object, str found"
     ]
    }
   ],
   "source": [
    "###CODE to do actual Pre-Processing for ['Text']:\n",
    "\n",
    "i=-1\n",
    "all_positive_words=[]\n",
    "all_negative_words=[]\n",
    "final_text=[]\n",
    "for text in final['Text']:\n",
    "    i = i+1\n",
    "    filtered_words=[]\n",
    "    text = allconvert(text)\n",
    "    for w in text.split():\n",
    "        if (w not in stop):\n",
    "            s =(sno.stem(w)).encode('utf8')\n",
    "            filtered_words.append(s)\n",
    "            if (final['Score'].values)[i]==1:\n",
    "                all_positive_words.append(s)\n",
    "            if (final['Score'].values)[i]==0:\n",
    "                all_negative_words.append(s)\n",
    "        else:\n",
    "            continue\n",
    "    str =b' '.join(filtered_words)\n",
    "    final_text.append(str)\n",
    "\n",
    "\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "###CODE to do actual Pre-Processing for ['Summary']:\n",
    "i=-1\n",
    "all_positive_words=[]\n",
    "all_negative_words=[]\n",
    "final_text=[]\n",
    "for text in final['Summary']:\n",
    "    i = i+1\n",
    "    filtered_words=[]\n",
    "    text = allconvert(text)\n",
    "    for w in text.split():\n",
    "        if (w not in stop):\n",
    "            s =(sno.stem(w)).encode('utf8')\n",
    "            filtered_words.append(s)\n",
    "            if (final['Score'].values)[i]==1:\n",
    "                all_positive_words.append(s)\n",
    "            if (final['Score'].values)[i]==0:\n",
    "                all_negative_words.append(s)\n",
    "        else:\n",
    "            continue\n",
    "    str = b' '.join(filtered_words)\n",
    "    final_text.append(str)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['Cleanedtext'] = final_text\n",
    "#final['Cleanedsummary'] = final_text\n",
    "\n",
    "conn = sqlite3.connect('final2.sqlite')\n",
    "c = conn.cursor()\n",
    "conn.text_factory = str\n",
    "\n",
    "\n",
    "##Using this attribute you can control what objects are returned for the TEXT data type.\n",
    "##By default, this attribute is set to unicode and the sqlite3 module will return Unicode objects for TEXT.\n",
    "##If you want to return bytestrings instead, you can set it to str.\n",
    "\n",
    "\n",
    "final.to_sql('Reviews',conn,schema=None,if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "final_counts = count_vect.fit_transform(final['Cleanedtext'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(final_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9564, 19416)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unigram, bigram,n - gram\n",
    "\n",
    "Motivation:::\n",
    "As we have all our positive and negative words separated we will try to find whether sequencing of words will make any sense or not.\n",
    "\n",
    "Both negative and positive has 'like' and 'love', so use N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Common Positive Word: [(b'like', 3487), (b'tast', 3136), (b'flavor', 2905), (b'good', 2875), (b'love', 2665), (b'great', 2640), (b'coffe', 2335), (b'product', 2104), (b'make', 1860), (b'food', 1657), (b'would', 1361), (b'realli', 1340), (b'tri', 1311), (b'time', 1302), (b'best', 1276), (b'use', 1210), (b'price', 1209), (b'find', 1195), (b'much', 1184), (b'order', 1174)]\n",
      "**************************************************\n",
      "Most Common Negative Word: [(b'tast', 906), (b'like', 863), (b'product', 748), (b'flavor', 555), (b'would', 506), (b'good', 406), (b'coffe', 402), (b'food', 371), (b'order', 346), (b'tri', 338), (b'even', 325), (b'dont', 319), (b'time', 315), (b'make', 296), (b'much', 286), (b'drink', 261), (b'realli', 260), (b'review', 258), (b'water', 250), (b'use', 242)]\n"
     ]
    }
   ],
   "source": [
    "freq_dist_positive = nltk.FreqDist(all_positive_words)\n",
    "freq_dist_negative = nltk.FreqDist(all_negative_words)\n",
    "print('Most Common Positive Word:',freq_dist_positive.most_common(20))\n",
    "print('*'*50)\n",
    "print('Most Common Negative Word:',freq_dist_negative.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count_vect_1 = CountVectorizer(ngram_range=(1,3)) ## It will give unigram,bigram and trigram all of them\n",
    "\n",
    "##Stopwords like 'not' should not be removed when doing N-grams\n",
    "\n",
    "#bi-gram, tri-gram and n-gram\n",
    "\n",
    "#removing stop words like \"not\" should be avoided before building n-grams\n",
    "# count_vect = CountVectorizer(ngram_range=(1,2))\n",
    "# please do read the CountVectorizer documentation http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "# you can choose these numebrs min_df=10, max_features=5000, of your choice\n",
    "count_vect_1 = CountVectorizer(ngram_range=(1,2))\n",
    "final_counts_1 = count_vect_1.fit_transform(final['Cleanedtext'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9564, 213094)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_counts_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF- IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_idf_vect = TfidfVectorizer(ngram_range=(1,2))\n",
    "final_tf_idf = tf_idf_vect.fit_transform(final['Cleanedtext'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tf_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tf_idf_vect.get_feature_names()\n",
    "features[10000:10010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source: https://buhrmann.github.io/tfidf-analysis.html\n",
    "##Fucntion to fetch top TF_IDF features\n",
    "\n",
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "top_tfidf_feats(final_tf_idf[2,:].toarray()[0],features,25)   ##to array is used because we can not print sparse matrix \n",
    "                                                              ##and it converts sparse matrix to dense matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word 2 Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "list_of_sentance=[]\n",
    "for sentance in final['Text']:\n",
    "    list_of_sentance.append(sentance.split())\n",
    "\n",
    "####If you use stemming and use encoding('utf-8') you need to decode it back for word2vec, \n",
    "##also stemming pre-processing not required for word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('good', 0.8708452582359314), ('wonderful', 0.8256304264068604), ('delicious', 0.792957067489624), ('perfect', 0.7846847772598267), ('nice', 0.7772462368011475), ('Great', 0.7187432646751404), ('healthy', 0.7084371447563171), ('fantastic', 0.6984565258026123), ('you.', 0.6946903467178345), ('quick', 0.6784365177154541)]\n",
      "==================================================\n",
      "[('best.', 0.926323413848877), ('seen.', 0.9088826179504395), ('K-cup.', 0.8918473124504089), ('popcorn', 0.8828972578048706), ('gum', 0.8807246685028076), ('K-cup', 0.862686812877655), ('had!', 0.8614288568496704), ('cheapest', 0.8607341647148132), ('tried!', 0.8582534790039062), ('blending', 0.8580650091171265)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "google_word2Vec = False\n",
    "Train_Model = True\n",
    "\n",
    "if google_word2Vec:\n",
    "    if os.path.isfile('GoogleNews-vectors-negative300.bin'):\n",
    "        w2v_model=KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "        print(w2v_model.wv('computer'))\n",
    "        print(w2v_model.wv.most_similar('worst'))\n",
    "    else:\n",
    "        print('File Does not exist')\n",
    "elif Train_Model:\n",
    "    w2v_model = Word2Vec(list_of_sentance,min_count=5,size=50, workers=4)\n",
    "    print(w2v_model.wv.most_similar('great'))\n",
    "    print('='*50)\n",
    "    print(w2v_model.wv.most_similar('worst'))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
